{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import platform\n",
    "import tensorflow as tf\n",
    "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
    "from tensorflow.python.saved_model import tag_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a64fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data generation notebook\n",
    "DATA_DBFS_ROOT_DIR = '/tmp/recommender/data'\n",
    " \n",
    "# You can change these as needed \n",
    "CACHE_PATH = 'file:///dbfs/tmp/recommender/converter_cache'\n",
    "CKPT_PATH = '/dbfs/tmp/recommender/model_ckpt'\n",
    "EXPORT_PATH = '/dbfs/tmp/recommender/model_export'\n",
    " \n",
    "# Enable mlflow autolog to log the metrics and model\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7173c352",
   "metadata": {},
   "source": [
    "# 1.1 Define the input columns\n",
    "The get_wide_and_deep_columns() function returns a tuple of (wide_columns, deep_columns) where each element is a list of tf.feature_column.\n",
    "\n",
    "Specify the name of the label column in LABEL_COLUMN.\n",
    "\n",
    "To use this reference solution with your own data, you need to implement the get_wide_and_deep_columns() function that returns the correct columns corresponding to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'label'\n",
    "NUMERIC_COLUMNS = [\n",
    "  'user_age',\n",
    "  'item_age',\n",
    "]\n",
    "CATEGORICAL_COLUMNS = [\n",
    "  'user_id',\n",
    "  'item_id',\n",
    "  'user_topic',\n",
    "  'item_topic',\n",
    "]\n",
    "HASH_BUCKET_SIZES = {\n",
    "  'user_id': 400,\n",
    "  'item_id': 2000,\n",
    "  'user_topic': 10,\n",
    "  'item_topic': 10,\n",
    "}\n",
    "EMBEDDING_DIMENSIONS = {\n",
    "    'user_id': 8,\n",
    "    'item_id': 16,\n",
    "    'user_topic': 3,\n",
    "    'item_topic': 3,\n",
    "}\n",
    " \n",
    "def get_wide_and_deep_columns():\n",
    "    wide_columns, deep_columns = [], []\n",
    "    \n",
    "    # embedding columns\n",
    "    for column_name in CATEGORICAL_COLUMNS:\n",
    "        categorical_column = tf.feature_column.categorical_column_with_identity(\n",
    "                              column_name, num_buckets=HASH_BUCKET_SIZES[column_name])\n",
    "        wrapped_column = tf.feature_column.embedding_column(\n",
    "                              categorical_column,\n",
    "                              dimension=EMBEDDING_DIMENSIONS[column_name],\n",
    "                              combiner='mean')\n",
    "        wide_columns.append(categorical_column)\n",
    "        deep_columns.append(wrapped_column)\n",
    "        \n",
    "    # age columns and cross columns\n",
    "    user_age = tf.feature_column.numeric_column(\"user_age\", shape=(1,), dtype=tf.float32)\n",
    "    item_age = tf.feature_column.numeric_column(\"item_age\", shape=(1,), dtype=tf.float32)       \n",
    "    user_age_buckets = tf.feature_column.bucketized_column(user_age, boundaries=[18, 35])\n",
    "    item_age_buckets = tf.feature_column.bucketized_column(item_age, boundaries=[18, 35])\n",
    "    age_crossed = tf.feature_column.crossed_column([user_age_buckets, item_age_buckets], 9)\n",
    "    wide_columns.extend([user_age_buckets, item_age_buckets, age_crossed])\n",
    "    deep_columns.extend([user_age, item_age])\n",
    "\n",
    "    # topic columns and cross columns\n",
    "    user_topic = tf.feature_column.categorical_column_with_identity(\n",
    "        \"user_topic\", num_buckets=HASH_BUCKET_SIZES[\"user_topic\"])\n",
    "    item_topic = tf.feature_column.categorical_column_with_identity(\n",
    "         \"item_topic\", num_buckets=HASH_BUCKET_SIZES[\"item_topic\"])       \n",
    "    topic_crossed = tf.feature_column.crossed_column([user_topic, item_topic], 30)\n",
    "    wide_columns.append(topic_crossed)\n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa213c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns, deep_columns = get_wide_and_deep_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb4433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ebd6af",
   "metadata": {},
   "source": [
    "## Define the Wide and Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass optimizer callables instead of optimizer objects to avoid this issue:\n",
    "# https://stackoverflow.com/questions/58108945/cannot-do-incremental-training-with-dnnregressor\n",
    "estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    # wide settings\n",
    "    linear_feature_columns=wide_columns,\n",
    "    linear_optimizer=tf.keras.optimizers.Ftrl,  # no brackets on purpose\n",
    "    # deep settings\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_hidden_units=[100, 50],\n",
    "    dnn_optimizer=tf.keras.optimizers.Adagrad,  # no brackets on purpose\n",
    "    # warm-start settings\n",
    "    model_dir=CKPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b0a20",
   "metadata": {},
   "source": [
    "## Create the custom metric\n",
    "This notebook uses the mean average precision at k as the evaluation metric.\n",
    "\n",
    "TensorFlow provides a built-in metric tf.compat.v1.metrics.average_precision_at_k. See tf.compat.v1.metrics.average_precision_at_k and this answer in stackoverflow to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27655731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/Recommendation/WideAndDeep/utils/metrics.py\n",
    "def map_custom_metric(features, labels, predictions):\n",
    "  user_ids = tf.reshape(features['user_id'], [-1])\n",
    "  predictions = predictions['probabilities'][:, 1]\n",
    " \n",
    "  # Processing unique user_ids, indexes and counts\n",
    "  # Sorting needed in case the same user_id occurs in two different places\n",
    "  sorted_ids = tf.argsort(user_ids)\n",
    "  user_ids = tf.gather(user_ids, indices=sorted_ids)\n",
    "  predictions = tf.gather(predictions, indices=sorted_ids)\n",
    "  labels = tf.gather(labels, indices=sorted_ids)\n",
    " \n",
    "  _, user_ids_idx, user_ids_items_count = tf.unique_with_counts(\n",
    "      user_ids, out_idx=tf.int64)\n",
    "  pad_length = 30 - tf.reduce_max(user_ids_items_count)\n",
    "  pad_fn = lambda x: tf.pad(x, [(0, 0), (0, pad_length)])\n",
    " \n",
    "  preds = tf.RaggedTensor.from_value_rowids(\n",
    "      predictions, user_ids_idx).to_tensor()\n",
    "  labels = tf.RaggedTensor.from_value_rowids(\n",
    "      labels, user_ids_idx).to_tensor()\n",
    " \n",
    "  labels = tf.argmax(labels, axis=1)\n",
    " \n",
    "  return {\n",
    "      'map': tf.compat.v1.metrics.average_precision_at_k(\n",
    "          predictions=pad_fn(preds),\n",
    "          labels=labels,\n",
    "          k=5,\n",
    "          name=\"streaming_map\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.add_metrics(estimator, map_custom_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a83a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2234d195",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f971f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(name):\n",
    "  return spark.read.format(\"delta\").load(f\"{DATA_DBFS_ROOT_DIR}/{name}\")\n",
    " \n",
    "train_df = load_df(\"user_item_interaction_train\")\n",
    "val_df = load_df('user_item_interaction_val')\n",
    "test_df = load_df('user_item_interaction_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39726bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_profile = load_df(\"item_profile\")\n",
    "user_profile = load_df(\"user_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5267468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features(df):\n",
    "  return df.join(item_profile, on='item_id', how=\"left\").join(user_profile, on='user_id', how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_w_features = join_features(train_df)\n",
    "val_df_w_features = join_features(val_df)\n",
    "test_df_w_features = join_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656469c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the cache directory\n",
    "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, CACHE_PATH)\n",
    " \n",
    "# Create the converters\n",
    "# Section 4: Train and evaluate the model uses these converters\n",
    "train_converter = make_spark_converter(train_df_w_features)    \n",
    "val_converter = make_spark_converter(val_df_w_features)\n",
    "test_converter = make_spark_converter(test_df_w_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f759bd",
   "metadata": {},
   "source": [
    "### Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db73e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the code snippet in this cell on new datasets without modification\n",
    "def to_tuple(batch):\n",
    "  \"\"\"\n",
    "  Utility function that converts the batch from the namedtuple type\n",
    "  to tuple type.\n",
    "  \"\"\"\n",
    "  feature = {\n",
    "    \"user_id\": batch.user_id,\n",
    "    \"user_age\": batch.user_age,\n",
    "    \"user_topic\": batch.user_topic,\n",
    "    \"item_id\": batch.item_id,\n",
    "    \"item_age\": batch.item_age,\n",
    "    \"item_topic\": batch.item_topic,\n",
    "  }\n",
    "  if hasattr(batch, \"label\"):\n",
    "    return feature, batch.label\n",
    "  return feature, None\n",
    " \n",
    "def get_input_fn(dataset_context_manager):\n",
    "  \"\"\"\n",
    "  Utility function that create the input function from the tf dataset returned by the\n",
    "  spark dataset converter.\n",
    "  \"\"\"\n",
    "  def fn():\n",
    "    return dataset_context_manager.__enter__().map(to_tuple)\n",
    "  return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = train_converter.make_tf_dataset()\n",
    "val_tf_dataset = val_converter.make_tf_dataset()\n",
    "test_tf_dataset = test_converter.make_tf_dataset()\n",
    " \n",
    "train_spec = tf.estimator.TrainSpec(input_fn=get_input_fn(train_tf_dataset), max_steps=1250)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=get_input_fn(val_tf_dataset))\n",
    " \n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ee55c",
   "metadata": {},
   "source": [
    "### Export trained model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ature_spec = tf.feature_column.make_parse_example_spec(\n",
    "    wide_columns + deep_columns\n",
    ")\n",
    "fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "    feature_spec\n",
    ")\n",
    "saved_model_path = estimator.export_saved_model(\n",
    "    export_dir_base=EXPORT_PATH,\n",
    "    serving_input_receiver_fn=fn\n",
    ").decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a19ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = {\n",
    "  # Get the path of the model logged in the current active run\n",
    "  \"model\": mlflow.get_artifact_uri() + '/model'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e10679",
   "metadata": {},
   "source": [
    "### Online Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pdf = pd.DataFrame(\n",
    "  {\n",
    "    \"user_id\": [1, 2],\n",
    "    \"item_id\": [[1, 2, 3], [4, 5, 6]],\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippet in this cell can be reused on new datasets with some modifications to match the feature types of your dataset. See https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "def serialize_example(input_pdf):\n",
    "  \"\"\"\n",
    "  Serialize a pandas DF with retrieved features to a proto (tf.train.Example).\n",
    "  \"\"\"\n",
    "  proto_tensors = []\n",
    "  for i in range(len(input_pdf)):\n",
    "    feature = dict()\n",
    "    for field in input_pdf:\n",
    "      if field in NUMERIC_COLUMNS:\n",
    "        feature[field] = tf.train.Feature(float_list=tf.train.FloatList(value=[input_pdf[field][i]]))\n",
    "      else: \n",
    "        feature[field] = tf.train.Feature(int64_list=tf.train.Int64List(value=[input_pdf[field][i]]))\n",
    " \n",
    "    # Create a Features message using tf.train.Example.\n",
    "    proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    proto_string = proto.SerializeToString()\n",
    "    proto_tensors.append(tf.constant([proto_string]))\n",
    "  return proto_tensors\n",
    " \n",
    "def sort_by_group(input_pdf, results_list):\n",
    "  \"\"\"\n",
    "  Sort the rows in input_pdf by the predicted scores in results_list.\n",
    "  \"\"\"\n",
    "  result_pdf = input_pdf.copy()\n",
    "  result_pdf['probabilities'] = [item['probabilities'][0, 1].numpy() for item in results_list]\n",
    "  return result_pdf \\\n",
    "    .sort_values(by='probabilities', ascending=False) \\\n",
    "    .groupby(\"user_id\") \\\n",
    "    .agg({'item_id': lambda x: x.to_list(), 'probabilities': lambda x: x.to_list()}) \\\n",
    "    .reset_index()\n",
    " \n",
    "# A PythonModel with custom inference logic\n",
    "class Recommender(mlflow.pyfunc.PythonModel):\n",
    "  \n",
    "  def load_context(self, context):\n",
    "    self.model = mlflow.tensorflow.load_model(context.artifacts[\"model\"])    \n",
    " \n",
    "  def predict(self, context, model_input):\n",
    "    proto = serialize_example(model_input)\n",
    "    results_list = []\n",
    "    for p in proto:\n",
    "      results_list.append(self.model(p))\n",
    "    return sort_by_group(model_input, results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21875fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function retrieves the user profile features and item profile features\n",
    "def retrieve_features(input_pdf):\n",
    "  input_df = spark.createDataFrame(input_pdf.explode('item_id'))\n",
    "  return join_features(input_df).toPandas().astype({'user_age': 'float', 'item_age': 'float'})\n",
    " \n",
    "print(\"Retrieving features...\")\n",
    "input_features_pdf = retrieve_features(input_pdf)\n",
    "input_features_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Conda environment for the new MLflow Model that contains all necessary dependencies.\n",
    "# Modify the following entries to match the environment you are using.\n",
    "conda_env = {\n",
    "    'channels': ['defaults'],\n",
    "    'dependencies': [\n",
    "      f'python={platform.python_version()}',\n",
    "      'pip',\n",
    "      {\n",
    "        'pip': [\n",
    "          'mlflow',\n",
    "          f'tensorflow-cpu=={tf.__version__}',\n",
    "          'tensorflow-estimator',\n",
    "        ],\n",
    "      },\n",
    "    ],\n",
    "    'name': 'recommender_env'\n",
    "}\n",
    "# Log the MLflow pyfunc PythonModel\n",
    "mlflow_pyfunc_model_path = \"recommender_mlflow_pyfunc\"\n",
    "mlflow.pyfunc.log_model(\n",
    "  artifact_path=mlflow_pyfunc_model_path, \n",
    "  python_model=Recommender(), \n",
    "  artifacts=artifacts,\n",
    "  conda_env=conda_env, \n",
    "  input_example=input_features_pdf, \n",
    "  registered_model_name=\"recommender\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
