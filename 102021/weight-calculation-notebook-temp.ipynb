{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'quantipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e4000a99146c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mquantipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mqp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'quantipy'"
     ]
    }
   ],
   "source": [
    "#quantipy for RIM and pandas for processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quantipy as qp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#parameters Initialization\n",
    "Population = \"69024\"\n",
    "Multiplier = \"5\"\n",
    "ScaleDownFactor = \"1\"\n",
    "Male = \"48.4\"\n",
    "Female = \"51.6\"\n",
    "Legacy = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Path to store the weighted data\n",
    "path = \"{}/processed/weighted/{}/{}/weighted_respondent.csv/\".format(generic_path,Year,Quarter)\n",
    "#Path to store backup files\n",
    "backup_path = \"{}/processed/weighted/{}/{}/weighted_respondent_backup\".format(generic_path,Year,Quarter)\n",
    "#Previously backuped files path\n",
    "#c_backup_path = \"{}/processed/weighted/{}/{}/weighted_respondent_backup/weighted_respondent.csv\".format(generic_path,Year,Quarter)\n",
    "mssparkutils.fs.mkdirs(backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#When reiterrating the process coppy files that are previously successful and create a backup  \n",
    "try:    \n",
    "    existing_files = mssparkutils.fs.ls(path)   \n",
    "except:\n",
    "    existing_files = None\n",
    "    \n",
    "#Delete the previous version of backup\n",
    "files = mssparkutils.fs.ls(backup_path)\n",
    "for file in files:\n",
    "    if(file.isDir == False):\n",
    "        mssparkutils.fs.rm(\"{}/{}\".format(backup_path, file.name))\n",
    "        #print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Backup previously executed files\n",
    "if existing_files is not None:\n",
    "    #mssparkutils.fs.cp(path, backup_path,True)\n",
    "    for file in existing_files:\n",
    "        mssparkutils.fs.cp(file.path, backup_path)\n",
    "        #print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# var definitions\n",
    "###########################\n",
    "\n",
    "POPULATION = int(Population)\n",
    "QUARTER = \"{}q{}\".format(Year,Quarter)\n",
    "\n",
    "# capping\n",
    "multiplier = int(Multiplier)\n",
    "scale_down_factor = int(ScaleDownFactor)\n",
    "male_count=float(Male)\n",
    "female_count=float(Female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#check installed components(python - packages/libraries)\n",
    "#import pkg_resources\n",
    "#for d in pkg_resources.working_set:\n",
    "#     print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Initializing the Source paths\n",
    "#only the postalcode is parameter not initialized\n",
    "\n",
    "#Card Data\n",
    "#Card_Source_Path = \"G:/Arimac Lanka/2018Q1/Card2018Q1.csv\"\n",
    "card_source_path = card_source_path = \"{}/curated/card/{}/{}/card.csv\".format(generic_path,Year,Quarter) # /part-00007-1a6bae43-ae54-4870-be63-205f66b430b5-c000.csv\"\n",
    "\n",
    "#respondant Data\n",
    "#Respondant_SourcePath = \"G:/Arimac Lanka/2018Q1/respondant.csv\"\n",
    "respondant_source_path = \"{}/curated/respondent/{}/{}/respondent.csv\".format(generic_path,Year,Quarter) #/part-00002-878212a3-89e7-4724-b8ff-e762fbcfe5d4-c000.csv\"\n",
    "\n",
    "#CardKind\n",
    "#cardkind_source_path = \"G:/Arimac Lanka/2018Q1/cardkind.csv\"\n",
    "cardkind_source_path = \"{}/curated/cardkind/{}/{}/cardkind.csv\".format(generic_path,Year, Quarter)\n",
    "\n",
    "#Postal Codes\n",
    "#PostalCodes_SourcePath = \"G:/Arimac Lanka/2018Q1/postal_codes.csv\"\n",
    "\n",
    "# Value kept hardcoded as only Postal Codes are generated for 2018/04 quarter\n",
    "postalCodes_source_path = \"{}/curated/vpp-mysqldb/postal-service/regions/{}/{}/postal_codes.csv\".format(generic_path,Year, Quarter)\n",
    "\n",
    "# legacy respondent for missing data\n",
    "if Legacy == \"True\":\n",
    "    legacy_repondent_source_path = \"{}/raw/legacy-pun/survey-response/respondent/{}/{}/respondent.csv\".format(generic_path,Year,Quarter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing targets source files\n",
    "\n",
    "#RegionTarget\n",
    "#Region Targets - Raw/2018/4/regiontargets.csv\n",
    "#region_codes = \"G:/Arimac Lanka/2018Q1/region-targets.csv\"\n",
    "region_codes_source_path = \"{}/curated/vpp-mysqldb/weight-service/region/{}/4/region-targets.csv\".format(generic_path,Year,Quarter)\n",
    "\n",
    "#source = \"G:/Arimac Lanka/2018Q1/household_comp-targets.csv\"\n",
    "#change agr to age after publishing and running the weights pipline\n",
    "age_targets_source = \"{}/curated/vpp-mysqldb/weight-service/age/{}/4/age-targets.csv\".format(generic_path,Year)\n",
    "household_comp_targets_source = \"{}/curated/vpp-mysqldb/weight-service/household_comp/{}/4/household_comp-targets.csv\".format(generic_path,Year)\n",
    "income_targets_source = \"{}/curated/vpp-mysqldb/weight-service/income/{}/4/income-targets.csv\".format(generic_path,Year)\n",
    "region_targets_source = \"{}/curated/vpp-mysqldb/weight-service/region/{}/4/region-targets.csv\".format(generic_path,Year)\n",
    "race_targets_source = \"{}/curated/vpp-mysqldb/weight-service/race/{}/4/race-targets.csv\".format(generic_path,Year,Quarter)\n",
    "amex_targets_source = \"{}/curated/vpp-mysqldb/weight-service/amexownership/{}/4/amex_ownership-targets.csv\".format(generic_path,Year)\n",
    "visa_targets_source = \"{}/curated/vpp-mysqldb/weight-service/visa_Ownership/{}/4/visa_ownership-targets.csv\".format(generic_path,Year)\n",
    "mastercard_targets_source = \"{}/curated/vpp-mysqldb/weight-service/mastercard_ownership/{}/4/mastercard_ownership-targets.csv\".format(generic_path,Year)\n",
    "debitcard_targets_source = \"{}/curated/vpp-mysqldb/weight-service/debit_card_ownership/{}/4/debit_card_ownership-targets.csv\".format(generic_path,Year)\n",
    "#discover_targets_source = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "microsoft": {
     "language": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "\n",
    "#Read from ADLS and load it as a Pandas DataFrame\n",
    "resps = spark.read.load(respondant_source_path, format = 'csv', sep = '|', header = True)\n",
    "cards = spark.read.load(card_source_path, format = 'csv',  sep = '|', header = True)\n",
    "po_codes = spark.read.load(postalCodes_source_path, format = 'csv', header = True)\n",
    "cardkind  = spark.read.load(cardkind_source_path, format = 'csv',  sep = '|', header = False)\n",
    "region_codes = spark.read.load(region_codes_source_path, format = 'csv', header = True)\n",
    "\n",
    "if Legacy == \"True\":\n",
    "    legacy_repondent = spark.read.load(legacy_repondent_source_path, format = 'csv', sep = '|', header = True)\n",
    "    legacy_repondent = legacy_repondent.toPandas()\n",
    "\n",
    "#print('Converting to Pandas.')\n",
    "#Pandas Conversion\n",
    "\n",
    "resps = resps.toPandas()\n",
    "cards = cards.toPandas()\n",
    "po_codes = po_codes.toPandas()\n",
    "cardkind = cardkind.toPandas()\n",
    "region_codes = region_codes.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "resps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "legacy_repondent.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "#Targets Dataframes\n",
    "\n",
    "#Read from ADLS and load it as a Pandas DataFrame\n",
    "age_targets = spark.read.load(age_targets_source, format = 'csv',  header = True)\n",
    "household_comp_targets = spark.read.load(household_comp_targets_source, format = 'csv', header = True)\n",
    "income_targets = spark.read.load(income_targets_source, format = 'csv', header = True)\n",
    "region_targets  = spark.read.load(region_targets_source, format = 'csv', header = True)\n",
    "race_targets = spark.read.load(race_targets_source, format = 'csv', header = True)\n",
    "amex_targets = spark.read.load(amex_targets_source, format = 'csv', header = True)\n",
    "visa_targets = spark.read.load(visa_targets_source, format = 'csv', header = True)\n",
    "mastercard_targets = spark.read.load(mastercard_targets_source, format = 'csv', header = True)\n",
    "debitcard_targets = spark.read.load(debitcard_targets_source, format = 'csv', header = True)\n",
    "\n",
    "#print('Converting to Pandas.')\n",
    "#Pandas Conversion\n",
    "\n",
    "age_targets = age_targets.toPandas()\n",
    "household_comp_targets = household_comp_targets.toPandas()\n",
    "income_targets = income_targets.toPandas()\n",
    "region_targets = region_targets.toPandas()\n",
    "race_targets = race_targets.toPandas()\n",
    "amex_targets = amex_targets.toPandas()\n",
    "visa_targets = visa_targets.toPandas()\n",
    "mastercard_targets = mastercard_targets.toPandas()\n",
    "debitcard_targets = debitcard_targets.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# helper method definition\n",
    "###########################\n",
    "#Categorising values\n",
    "\n",
    "# maps age to its predefined AgeBands\n",
    "def getAgeBandID(age):\n",
    "    \"\"\" This method will return the ageband(A measurement) \n",
    "        according to the responants age(Input).\n",
    "        ageBands : 0 - 9\n",
    "        if an ageBand == 0 its a garbage value\n",
    "    \"\"\"\n",
    "    if(type(age) == str):\n",
    "        return  0\n",
    "\n",
    "    ageBand = 0\n",
    "    if(age <= 0):\n",
    "        ageBand = 0\n",
    "    elif (age < 25):\n",
    "        ageBand = 1\n",
    "    elif (age < 30):\n",
    "        ageBand = 2\n",
    "    elif (age < 35):\n",
    "        ageBand = 3\n",
    "    elif (age < 40):\n",
    "        ageBand = 4\n",
    "    elif (age < 45):\n",
    "        ageBand = 5\n",
    "    elif (age < 50):\n",
    "        ageBand = 6\n",
    "    elif (age < 55):\n",
    "        ageBand = 7\n",
    "    elif (age < 60):\n",
    "        ageBand = 8\n",
    "    elif (age >=60 and age<=120):\n",
    "        ageBand = 9\n",
    "    \n",
    "    return ageBand\n",
    "\n",
    "#returns a list of income mapping\n",
    "def getIncomeMapping(dataf):\n",
    "    pun_limit = dataf.PUN_limit.astype(int).values.tolist()\n",
    "    pun_limit = list(set(pun_limit))\n",
    "    pun_limit.sort()\n",
    "\n",
    "    return pun_limit    \n",
    "\n",
    "#checks tthe inome band of respondant\n",
    "def getIncomeBandID(income, mapping):\n",
    "    incomeBand = 0\n",
    "    if (income > mapping[-1]):\n",
    "        incomeBand = len(mapping)\n",
    "    else:\n",
    "        for i in range(len(mapping)):\n",
    "            if (income < mapping[i]):\n",
    "                incomeBand = i + 1\n",
    "                break\n",
    "    return incomeBand\n",
    "\n",
    "\n",
    "# maps race to its defined codes\n",
    "def getRaceID(race):\n",
    "    \"\"\"This method will return the race of the respondant\n",
    "        outputs = race : 1 - 4\n",
    "        if the race is not in the predefined codes \n",
    "        then it will return 0\"\"\"\n",
    "    raceBand = [1,2,3,4]\n",
    "    \n",
    "    if (race == 5):\n",
    "        race = 3\n",
    "    elif (race == 3 or race == 9):\n",
    "        race = 4\n",
    "    elif(race not in raceBand):\n",
    "        race = 0\n",
    "    return race\n",
    "\n",
    "# goes through cardKind codes to get whether a certain type(Visa, MasterCard...) are owned by the respondent\n",
    "def getCardOwnership(cardList, cardKindMapping):\n",
    "    \"\"\" This will return 1 \n",
    "            if the respondant has a card and \n",
    "            2 for not having a card\"\"\"\n",
    "    \n",
    "    cardowernership = 0\n",
    "    for j in cardList:\n",
    "        if (j in cardKindMapping):\n",
    "            cardowernership = 1\n",
    "        else:\n",
    "            cardowernership = 2\n",
    "    \n",
    "    return cardowernership\n",
    "\n",
    "#This method will return the corresponding cardmapping for the given provider \n",
    "def getCardkindMapping(cardkind_DF, providerID):\n",
    "    \"\"\"Returns a list of cardkinds for the given provider\n",
    "       input : cardkindDataframe(containing the card provider, list of cardkinds)\n",
    "               providerID - desired provider\n",
    "       output : List of cardKinds\"\"\"\n",
    "    cardkind_list = []\n",
    "    for _, provider in cardkind_DF.iterrows(): \n",
    "        for p_code in provider['providerCode']:\n",
    "            if(providerID == p_code):\n",
    "                for j in provider['cardkindCode']:\n",
    "                    cardkind_list.append(j)\n",
    "    \n",
    "    return cardkind_list\n",
    "\n",
    "\n",
    "# checks paymentType attribute for Debit cards\n",
    "def getDebitCardMap(cardkindDF):\n",
    "    \"\"\"This method will return the debit card mapping details\"\"\"\n",
    "    debitCardlist = []\n",
    "    \n",
    "    group_paymentType = cardkindDF.groupby('paymentType')\n",
    "    cardpayment_type_data = group_paymentType['paymentType','cardKindCode'].agg(lambda x: set(x))\n",
    "    \n",
    "    cardpay_dict = cardpayment_type_data.to_dict()\n",
    "    cardpayment_type_data = pd.DataFrame(cardpay_dict)\n",
    "    \n",
    "    for _, i in cardpayment_type_data.iterrows():\n",
    "        for paymentType in i['paymentType']:\n",
    "            if(paymentType == 2):\n",
    "                for cardKind in i['cardKindCode']:\n",
    "                    debitCardlist.append(cardKind)\n",
    "\n",
    "    return debitCardlist\n",
    "\n",
    "def getHouseHoldComp(lAgmt, mSts, lwr, hsize, chCode, gender):\n",
    "    #check for null values\n",
    "    #lAgmt   = i['livingArrangementCode']\n",
    "    #mSts    = i['maritalStatusCode']\n",
    "    #lwr     = i['livingWithRelativeCode']\n",
    "    #hsize   = i['householdSizeCode']\n",
    "    #chCode  = i['childrenCodes']\n",
    "    #gender  = i['genderCode']\n",
    "    Comp = 0\n",
    "\n",
    "    #Male/Female Only\n",
    "    if(hsize == 1):\n",
    "        if(mSts == 2 or mSts == 3):\n",
    "            #Male only\n",
    "            if(gender == 1):\n",
    "                Comp = 1\n",
    "            \n",
    "            #Female only\n",
    "            elif(gender == 2):\n",
    "                Comp = 2\n",
    "                \n",
    "    elif(hsize == 2):\n",
    "        #base logic for roommates\n",
    "        if(lwr == 1 and lAgmt == 2):\n",
    "            Comp = 3\n",
    "\n",
    "        #Hus/Wife only             \n",
    "        elif(mSts == 1):\n",
    "            Comp = 4\n",
    "            \n",
    "    #Hus/Wife \n",
    "    elif(hsize > 2):\n",
    "        #check the correct codes regarding the children\n",
    "        #childeren < 18\n",
    "        if(chCode == 5 or chCode == 3):\n",
    "            Comp = 5\n",
    "            \n",
    "        #children 18+\n",
    "        elif(chCode == 4):\n",
    "            Comp = 6\n",
    "            \n",
    "        #base logic for roommates\n",
    "        #roommates may own the place\n",
    "        elif(lwr == 1 and lAgmt == 2):\n",
    "            Comp = 3\n",
    "            \n",
    "    #If all conditions fail\n",
    "    else:\n",
    "        Comp = 7\n",
    "    \n",
    "    return Comp\n",
    "\n",
    "#Method will drop all the colums that we don't need when there are many cols in a Dataframe\n",
    "def notNeededcol(datF, colNotDrop):\n",
    "    \"\"\"This method will drop the unnessary columns in a dataframe\n",
    "        input : dataF     - Dataframe\n",
    "              : colNotDop - List of columns not to drop\"\"\"\n",
    "    col = datF.columns\n",
    "    coltoDrop = []\n",
    "    for i in range(len(col)):\n",
    "        if col[i] not in colNotDrop:\n",
    "            coltoDrop.append(col[i])\n",
    "    \n",
    "    if coltoDrop != None:\n",
    "        datF.drop(columns = coltoDrop, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Clean Cardkind \n",
    "cardkind = pd.DataFrame(cardkind)\n",
    "cardkind.columns = ['cardKind', 'cardKindCode', 'providerCode', 'paymentType', 'quarter']\n",
    "cardkind.drop(columns =['quarter'],inplace=True)\n",
    "cardkind = pd.DataFrame(cardkind)  \n",
    "cardkind[['cardKindCode', 'paymentType', 'providerCode']] = cardkind[['cardKindCode', 'paymentType', 'providerCode']].apply(pd.to_numeric)\n",
    "cardkind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Transformation for cardkind Mapping\n",
    "group_cardkind_Data = cardkind.groupby('providerCode')\n",
    "card_kind_data = group_cardkind_Data['providerCode','cardKindCode'].agg(lambda x: set(x))\n",
    "card_kind_data.columns = ['providerCode', 'cardkindCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create cardprovider and the related cardkind maping\n",
    "cardkind_dict = card_kind_data.to_dict()\n",
    "card_kind_data = pd.DataFrame(cardkind_dict)\n",
    "card_kind_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get cardkind codes for each card criteria\n",
    "VisaMapping = getCardkindMapping(card_kind_data, 4)\n",
    "AmexMapping = getCardkindMapping(card_kind_data, 1)\n",
    "MasterCardMapping = getCardkindMapping(card_kind_data, 3)\n",
    "DiscoverMapping = getCardkindMapping(card_kind_data, 2)\n",
    "\n",
    "incomeMapping = getIncomeMapping(income_targets)\n",
    "\n",
    "#Since most of the payment types are null, we have to get the debitcard kind mapping through the raw data(curated data is not yet set) \n",
    "DebitCardMapping = getDebitCardMap(cardkind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#regions Transformation\n",
    "region_codes.columns = ['region', 'regionCode', 'gender', 'quarter', 'targets']\n",
    "region_codes.drop(columns=['gender', 'quarter', 'targets'], inplace=True)\n",
    "region_codes.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Define the column that we need to keep in the dataframe\n",
    "rescolnotdrop = ['id', 'age', 'genderCode', 'maritalStatusCode', 'raceCode','postalCode', 'livingArrangementCode','livingWithRelativeCode', 'householdSizeCode', 'householdIncomeCode','childrenCodes', 'empStatusCode','weight']\n",
    "cardscolnotdrop = ['userId','cardKindCode','paymentTypeCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Removing the columns we don't need\n",
    "notNeededcol(resps,rescolnotdrop)\n",
    "notNeededcol(cards,cardscolnotdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Setting column names\n",
    "cards['cardKindCode'] = cards['cardKindCode'].apply(pd.to_numeric) \n",
    "cards.columns = ['id','paymentTypeCode','cardKindCode']\n",
    "cards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Merge respondants and card details(inner join)to find thecardkind code\n",
    "data = resps.merge(cards,how='inner',on=\"id\",validate=\"one_to_many\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Clean null values and change data types\n",
    "data.fillna(0, inplace=True)\n",
    "data.replace('NA', 0, inplace = True)\n",
    "data[['age', 'genderCode', 'maritalStatusCode', 'raceCode', 'postalCode', 'empStatusCode','livingArrangementCode','livingWithRelativeCode','householdSizeCode','householdIncomeCode', 'childrenCodes']] = data[['age','genderCode', 'maritalStatusCode', 'raceCode', 'postalCode', 'empStatusCode','livingArrangementCode','livingWithRelativeCode','householdSizeCode','householdIncomeCode', 'childrenCodes']].apply(pd.to_numeric)\n",
    "#data = data.astype({'livingArrangementCode' : np.int64, 'livingWithRelativeCode':np.int64, 'paymentTypeCode': np.int64, 'childrenCodes': np.int64})\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create a list of cardkindcodes for each respondant \n",
    "#Group by id\n",
    "groupData = data.groupby('id')\n",
    "#group by cardKindcode and add as a list\n",
    "carddata = groupData[[\"cardKindCode\",\"paymentTypeCode\"]].agg(lambda x: set(x))\n",
    "data = data.merge(carddata, how='inner', on=\"id\")\n",
    "data.drop(columns=['cardKindCode_x','paymentTypeCode_x'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def getLegactHouseHoldComp(df,eid):\n",
    "    comp = df[(df[\"userId\"] == eid)][\"householdComposition\"].values\n",
    "    print(comp)\n",
    "    if len(comp)>0:\n",
    "        return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "getLegactHouseHoldComp(legacy_repondent,'10bcb362-b194-11eb-b12f-acde48001122')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the respondant data according to the criteria \n",
    "\n",
    "def getHouseCompFromSource(eid, legacy):\n",
    "    if legacy == \"True\":\n",
    "        return getLegactHouseHoldComp(legacy_repondent,eid)\n",
    "    else:\n",
    "        return getHouseHoldComp(emp['livingArrangementCode'],emp['maritalStatusCode'],emp['livingWithRelativeCode'], emp['householdSizeCode'], emp['childrenCodes'],emp['genderCode'] )\n",
    "\n",
    "newRes = []\n",
    "\n",
    "for _, emp in data.iterrows():\n",
    "    values = [\n",
    "        emp['id'],\n",
    "        emp['genderCode'],\\\n",
    "        getHouseCompFromSource(emp['id'], Legacy),\\\n",
    "        getAgeBandID(emp['age']),\\\n",
    "        getIncomeBandID(emp['householdIncomeCode'], incomeMapping),\\\n",
    "        emp['postalCode'],\\\n",
    "        getRaceID(emp['raceCode']),\\\n",
    "        getCardOwnership(emp['cardKindCode_y'], AmexMapping),\\\n",
    "        getCardOwnership(emp['cardKindCode_y'], VisaMapping),\\\n",
    "        getCardOwnership(emp['cardKindCode_y'], MasterCardMapping),\\\n",
    "        getCardOwnership(emp['cardKindCode_y'], DebitCardMapping),\\\n",
    "        getCardOwnership(emp['cardKindCode_y'], DiscoverMapping)\n",
    "        ]\n",
    "    newRes.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns\n",
    "res = pd.DataFrame(newRes)\n",
    "res.drop_duplicates(inplace=True)\n",
    "res.columns = ['respondentId', 'genderCode','household_comp', 'age', 'income', 'postalCode', 'race', 'amex', 'visa', 'mastercard', 'debit_card', 'discover']\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the target region names according to the region names that are in the postal file\n",
    "region_codes.replace({'Mid Atlantic', 'East North Central', 'West North Central','East South Central','West South Central'},{'Middle Atlantic','E. North Central','W. North Central','E. South Central','W. South Central'},inplace=True)\n",
    "region_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged dataframe which has both postal codes and region codes\n",
    "po_region = po_codes.merge(region_codes, how=\"inner\", on='region')\n",
    "po_region.drop_duplicates(inplace = True)\n",
    "po_region.drop(columns = ['region','state','state_code'],inplace=True)\n",
    "po_region.columns = ['postalCode','regionCode']\n",
    "\n",
    "#append a data to map null values\n",
    "po_region.append({'postalCode': 0, 'regionCode': 0},ignore_index=True)\n",
    "po_region = po_region.apply(pd.to_numeric) \n",
    "po_region.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge respondants data to connect the region codes  \n",
    "resps = res.merge(po_region, how='left',on='postalCode')\n",
    "resps.drop(columns = 'postalCode', inplace =True)\n",
    "resps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace null values with 0 and change the regionCode type to int\n",
    "resps.fillna(value=0, axis = 0, inplace=True)\n",
    "resps['regionCode'] = resps['regionCode'].astype(np.int64, errors='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If any of the data has a garbage value seperate it from the original data\n",
    "reslist = resps.values.tolist()\n",
    "#store all the erronous values in this list\n",
    "error_res = []\n",
    "\n",
    "for i in range(len(reslist)):\n",
    "   for j in range(len(reslist[i])):\n",
    "      if (reslist[i][j] == 0):\n",
    "         error_res.append(reslist[i])\n",
    "         break\n",
    "\n",
    "#error_respondants\n",
    "error_responants = pd.DataFrame(error_res)\n",
    "error_responants.columns = ['respondentId', 'gender', 'household_comp', 'age', 'income', 'race', 'amex', 'visa', 'mastercard', 'debit_card', 'discover', 'region']\n",
    "error_responants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "resps.replace(to_replace=0, value=np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert any garbage values to NaN and drop it\n",
    "\n",
    "resps.dropna(how='any',axis=0, inplace= True)\n",
    "resps.columns = ['respondentId', 'gender', 'household_comp', 'age', 'income', 'race', 'amex', 'visa', 'mastercard', 'debit_card', 'discover', 'region']\n",
    "resps.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "resps['household_comp'] = resps['household_comp'].astype(int) \n",
    "resps['region'] = resps['region'].astype(int) \n",
    "resps['race'] = resps['race'].astype(int) \n",
    "resps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#this method will return the targets that needs to be achieved for a target\n",
    "def retrieveDictFromDatabase(targetDF, gender_id, identifier):      \n",
    "        targetDF['value'] = targetDF['value'].apply(pd.to_numeric) \n",
    "        col_not_to_drop = ['name', 'gender_id', 'band_code', 'value']\n",
    "        notNeededcol(targetDF, col_not_to_drop)\n",
    "\n",
    "        targets = {identifier: {}}\n",
    "        queryDF = targetDF.query('gender_id == @gender_id')\n",
    "\n",
    "        for _, i in queryDF.iterrows():\n",
    "            try:\n",
    "                i[3] = rount(i[3],2)\n",
    "            except:\n",
    "                pass\n",
    "            targets[identifier][int(i[1])] = i[3]\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Targets dictionaries\n",
    "# retrive population proportion values for each criteria from weights database\n",
    "household_comp_male_targets = retrieveDictFromDatabase(household_comp_targets, \"1\", \"household_comp\")\n",
    "household_comp_female_targets = retrieveDictFromDatabase(household_comp_targets, \"2\", \"household_comp\")\n",
    "age_male_targets = retrieveDictFromDatabase(age_targets, \"1\", \"age\")\n",
    "age_female_targets = retrieveDictFromDatabase(age_targets, \"2\", \"age\")\n",
    "income_male_targets = retrieveDictFromDatabase(income_targets, \"1\", \"income\")\n",
    "income_female_targets = retrieveDictFromDatabase(income_targets, \"2\", \"income\")\n",
    "region_male_targets = retrieveDictFromDatabase(region_targets, \"1\", \"region\")\n",
    "region_female_targets = retrieveDictFromDatabase(region_targets, \"2\", \"region\")\n",
    "race_male_targets = retrieveDictFromDatabase(race_targets, \"1\", \"race\")\n",
    "race_female_targets = retrieveDictFromDatabase(race_targets, \"2\", \"race\")\n",
    "amex_male_targets = retrieveDictFromDatabase(amex_targets, \"1\", \"amex\")\n",
    "amex_female_targets = retrieveDictFromDatabase(amex_targets, \"2\", \"amex\")\n",
    "visa_male_targets = retrieveDictFromDatabase(visa_targets, \"1\", \"visa\")\n",
    "visa_female_targets = retrieveDictFromDatabase(visa_targets, \"2\", \"visa\")\n",
    "mastercard_male_targets = retrieveDictFromDatabase(mastercard_targets, \"1\", \"mastercard\")\n",
    "mastercard_female_targets = retrieveDictFromDatabase(mastercard_targets, \"2\", \"mastercard\")\n",
    "debitcard_male_targets = retrieveDictFromDatabase(debitcard_targets, \"1\", \"debit_card\")\n",
    "debit_card_female_targets = retrieveDictFromDatabase(debitcard_targets, '2', \"debit_card\")\n",
    "#discover_male_targets = retrieveDictFromDatabase(\"discover_ownership\", \"1\", \"discover\")\n",
    "#discover_female_targets = retrieveDictFromDatabase(\"discover_ownership\", \"2\", \"discover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "len(resps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weight = (POPULATION/resps.shape[0]) * multiplier\n",
    "print('max weight is {}'.format(max_weight))\n",
    "\n",
    "# define weighting scheme here\n",
    "scheme = qp.Rim('2019', total=POPULATION, convcrit=0.000001, cap=max_weight)\n",
    "\n",
    "# add filters based on gender\n",
    "male_filter = \"gender == 1\"\n",
    "female_filter  = \"gender == 2\"\n",
    "\n",
    "# set gender specific targets\n",
    "# scheme.add_group(name='male', filter_def=male_filter, targets=male_targets)\n",
    "# scheme.add_group(name='female', filter_def=female_filter, targets=female_targets)\n",
    "scheme.add_group(name='male', filter_def=male_filter, targets=[\n",
    "    household_comp_male_targets,\n",
    "    age_male_targets,\n",
    "    race_male_targets,\n",
    "    income_male_targets,\n",
    "    region_male_targets,\n",
    "    amex_male_targets,\n",
    "    visa_male_targets,\n",
    "    mastercard_male_targets,\n",
    "    # discover_male_targets,\n",
    "    debitcard_male_targets\n",
    "])\n",
    "scheme.add_group(name='female', filter_def=female_filter, targets=[\n",
    "    household_comp_female_targets,\n",
    "    age_female_targets,\n",
    "    race_female_targets,\n",
    "    income_female_targets,\n",
    "    region_female_targets,\n",
    "    amex_female_targets,\n",
    "    visa_female_targets,\n",
    "    mastercard_female_targets,\n",
    "    # discover_male_targets,\n",
    "    debit_card_female_targets\n",
    "])\n",
    "\n",
    "# set female and male group targets\n",
    "scheme.group_targets({\n",
    "    'male': male_count,\n",
    "    'female': female_count\n",
    "})\n",
    "\n",
    "# weighting scheme is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = qp.DataSet(QUARTER + \" Respondents\")\n",
    "df.from_components(resps)\n",
    "#resp_19.to_csv(\"G:/weights_outputs/2019_1_res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the weighting\n",
    "df.weight(scheme, weight_name='weight', unique_key='respondentId', inplace=True, report=True, path_report=\"2019 Q1\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.data().copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick cehck on gender based target proportions\n",
    "gender_group = data.groupby(by='gender').agg({\"weight\": 'sum'})\n",
    "gender_group['% weight'] = gender_group.weight * 100 / gender_group.weight.sum()\n",
    "gender_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify by race targets    \n",
    "race_group = data.groupby(by=['gender', 'race']).agg({\"weight\": 'sum', \"respondentId\": 'count'})\n",
    "calc_val = (race_group.weight/ race_group.weight.sum()) * 200\n",
    "try:\n",
    "    round(calc_val,2)\n",
    "except:\n",
    "    pass\n",
    "race_group['% weight'] = calc_val\n",
    "race_group['% unweighted'] = (race_group.respondentId/ race_group.respondentId.sum()) * 200\n",
    "race_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify by age targets    \n",
    "age_group = data.groupby(by=['gender', 'age']).agg({\"weight\": 'sum', \"respondentId\": 'count'})\n",
    "calc_val_2 = (age_group.weight/ age_group.weight.sum()) * 200\n",
    "try:\n",
    "    round(calc_val_2,2)\n",
    "except:\n",
    "    pass\n",
    "age_group['% weight'] = calc_val_2\n",
    "age_group['% unweighted'] = (age_group.respondentId/ age_group.respondentId.sum()) * 200\n",
    "age_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify by age targets    \n",
    "age_group = data.groupby(by=['gender', 'age']).agg({\"weight\": 'sum', \"respondentId\": 'count'})\n",
    "calc_val_3 = (age_group.weight/ age_group.weight.sum()) * 200\n",
    "try:\n",
    "    round(calc_val_3, 2)\n",
    "except:\n",
    "    pass\n",
    "age_group['% weight'] = calc_val_3\n",
    "age_group['% unweighted'] = (age_group.respondentId/ age_group.respondentId.sum()) * 200\n",
    "age_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse weight\n",
    "w = pd.DataFrame(df.data().weight)\n",
    "w.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#capping\n",
    "threshold = max_weight\n",
    "def capping(max_thresh, resp_weight):\n",
    "    resp_weight = float(resp_weight)\n",
    "    if (resp_weight > max_thresh):\n",
    "        return max_thresh\n",
    "    else:\n",
    "        return resp_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data['weight'] = df.data().apply(lambda row: capping(threshold,str(row['weight'])),axis =1)\n",
    "data.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = data.groupby(by='gender').agg({'weight': 'sum'})\n",
    "calc_val_4 = gender.weight * 100/gender.weight.sum()\n",
    "try:\n",
    "    round(calc_val_4, 0)\n",
    "except:\n",
    "    pass\n",
    "gender['%'] = calc_val_4\n",
    "gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = data.groupby(by=['gender', 'race']).agg({'weight': 'sum'})\n",
    "calc_val_5 =gender.weight * 200/gender.weight.sum()\n",
    "try:\n",
    "    round(calc_val_5, 2)\n",
    "except:\n",
    "    pass\n",
    "gender['%'] = calc_val_5\n",
    "gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append two data frames and set the weighting 0 for the erronous values \n",
    "error_responants['weight'] = 0\n",
    "error_responants['@1'] = 1.0\n",
    "error_responants.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "error_responants['weight'] = error_responants['weight'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding two data frames\n",
    "data = data.append(error_responants, ignore_index = True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "data_df =spark.createDataFrame(data)\n",
    "\n",
    "data_df = data_df.na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Dumping Row Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "#path = \"{}/processed/weighted/{}/{}/weighted_respondent.csv\".format(generic_path,Year,Quarter)\n",
    "\n",
    "# Save as Table\n",
    "#data_df.write.saveAsTable(\"Weighted_respondent_{}_{}\".format(Year,Quarter))\n",
    "\n",
    "# Dump to a csv\n",
    "data_df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
