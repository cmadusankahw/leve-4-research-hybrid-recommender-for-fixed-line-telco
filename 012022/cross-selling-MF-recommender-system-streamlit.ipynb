{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f0d72f",
      "metadata": {
        "gather": {
          "logged": 1646538295907
        }
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.9.2 64-bit' requires ipykernel package.\n",
            "Run the following command to install 'ipykernel' into the Python environment. \n",
            "Command: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from ast import literal_eval\n",
        "from surprise import *\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import train_test_split\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323386df",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if 'total' not in st.session_state:\n",
        "    st.session_state['total'] = 300\n",
        "\n",
        "\n",
        "st.header(\"Telco Recs - Telecommunication Service Packages recommender\")\n",
        "st.subheader(\"Recommendations predicteed using collabarative Filtering approaches\")\n",
        "\n",
        "st.sidebar.subheader(\"Settings\")\n",
        "st.sidebar.selectbox(\"Select a Recommender Approach\", (\"Cross-Selling recommendations\", \"Up-selling Recommendations\"))\n",
        "st.sidebar.selectbox(\"Select a Recommender Model to Score\", (\"Similarity based Collobarative Filtering\",\"Wide and Deep Learning Model\", \"Factorization Machines\"))\n",
        "sel_algos = st.sidebar.multiselect(\n",
        "     'Select Recommendation Algorithms to Score',\n",
        "     ['Matrix Factorization', 'SVD', 'Cosine-Similarity', 'KNN with Means','slopeOne'],\n",
        "     ['SVD'])\n",
        "st.sidebar.info(\"Models you have selected: \", sel_algos)\n",
        "\n",
        "st.info(\"This recommendation model will predict cross-selling recommendations for a given dataset using **Cosine-Similarity**, **Matrix Factorization**, **SVD**, KNN and **slopeOne** algorithms. Automated Data processing will trigger including Data cleaning, Null handeling and feature Selection and models will provide real time predictions.\")\n",
        "\n",
        "img=Image.open(\"data/fm-data/crosssell.png\")\n",
        "\n",
        "st.image(img,use_column_width = 'always')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "568c9bc0",
      "metadata": {
        "gather": {
          "logged": 1645981122484
        }
      },
      "outputs": [],
      "source": [
        "up = st.text_input('Enter path to User Profile', 'data/User_Profile_Null_Handled.csv')\n",
        "st.write('User Profile dataset path entered:   ', up)\n",
        "st.text(\"\")\n",
        "pp = st.text_input('Enter path to Product Profile', 'data/Product_Profile.csv')\n",
        "st.write('Product Profile dataset path entered:   ', pp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9c27471",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_interaction_matrix(data):\n",
        "    interactions = data.groupby('ACCOUNT_NUM.hash').count()['package']\n",
        "    plt.hist(interactions,bins=20)\n",
        "    plt.show()\n",
        "\n",
        "    # create the user item matrix using the ratings dataset - Hint: try using pivot function \n",
        "    interactions_metrix = data.pivot_table(index=\"ACCOUNT_NUM.hash\", columns=\"package\", values=\"ratings\",aggfunc=np.sum)\n",
        "    # replace all the missing values with zero\n",
        "    return interactions_metrix.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4bdd23",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_splitter(data):\n",
        "    reader = Reader(rating_scale=(0, 1))\n",
        "    data_model = Dataset.load_from_df(data, reader)\n",
        "    return data_model,train_test_split(data_model, test_size=.20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e392787",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_Iu(uid):\n",
        "    \"\"\"Return the number of items rated by given user\n",
        "    \n",
        "    Args:\n",
        "        uid: The raw id of the user.\n",
        "    Returns:\n",
        "        The number of items rated by the user.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
        "    except ValueError:  # user was not part of the trainset\n",
        "        return 0\n",
        "    \n",
        "def get_Ui(iid):\n",
        "    \"\"\"Return the number of users that have rated given item\n",
        "    \n",
        "    Args:\n",
        "        iid: The raw id of the item.\n",
        "    Returns:\n",
        "        The number of users that have rated the item.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
        "    except ValueError:  # item was not part of the trainset\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6bcd07",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_n(predictions, n=10):\n",
        "    \n",
        "    # First map the predictions to each user.\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est, true_r))\n",
        "\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:5]\n",
        "        \n",
        "\n",
        "    return top_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1e7633",
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate NDCG\n",
        "def ndcg(y_true, y_pred, k=None, powered=False):\n",
        "    def dcg(scores, k=None, powered=False):\n",
        "        if k is None:\n",
        "            k = scores.shape[0]\n",
        "        if not powered:\n",
        "            ret = scores[0]\n",
        "            for i in range(1, k):\n",
        "                ret += scores[i] / np.log2(i + 1)\n",
        "            return ret\n",
        "        else:\n",
        "            ret = 0\n",
        "            for i in range(k):\n",
        "                ret += (2 ** scores[i] - 1) / np.log2(i + 2)\n",
        "            return ret\n",
        "    \n",
        "    ideal_sorted_scores = np.sort(y_true)[::-1]\n",
        "    ideal_dcg_score = dcg(ideal_sorted_scores, k=k, powered=powered)\n",
        "    \n",
        "    pred_sorted_ind = np.argsort(y_pred)[::-1]\n",
        "    pred_sorted_scores = y_true[pred_sorted_ind]\n",
        "    dcg_score = dcg(pred_sorted_scores, k=k, powered=powered)\n",
        "    \n",
        "    return dcg_score / ideal_dcg_score\n",
        "\n",
        "def ndcg1(y_true, y_pred, k=None):\n",
        "    return ndcg(y_true, y_pred, k=k, powered=False)\n",
        "\n",
        "def ndcg2(y_true, y_pred, k=None):\n",
        "    return ndcg(y_true, y_pred, k=k, powered=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b01acb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def top_n_pred(predictions):\n",
        "    top_n = get_top_n(predictions, n=3)\n",
        "    #print(top_n)\n",
        "    users_est = defaultdict(list)\n",
        "    users_true=defaultdict(list)\n",
        "    rec_for_user=defaultdict(list)\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        users_est[uid].append([est for (_, est,_) in user_ratings])\n",
        "        users_true[uid].append([true_r for (_,_,true_r) in user_ratings])\n",
        "        rec_for_user[uid].append([iid for (iid,_,_) in user_ratings])\n",
        "    return top_n, users_est, users_true, rec_for_user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42f468b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_ndcg(users_true,users_est):\n",
        "    ndcg_list=[]\n",
        "    for uid in top_n:\n",
        "        \n",
        "        for i in users_true[uid]:\n",
        "            y_true=np.asarray(i)#.reshape(-1,1)\n",
        "        for i in users_est[uid]:\n",
        "            y_pred=np.asarray(i)#.reshape(-1,1)\n",
        "        \n",
        "            ndcg_list.append(ndcg1(y_true, y_pred, k=None))\n",
        "\n",
        "    ndcg_list = [i for i in ndcg_list if str(i) != 'nan']\n",
        "    ndgc_rate = np.mean(ndcg_list)\n",
        "    return ndcg_list, ndgc_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57ae822",
      "metadata": {},
      "source": [
        "## Streamlit code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "e03ba629-704a-4136-9474-665f2db80461",
      "metadata": {
        "gather": {
          "logged": 1645981122817
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if st.button(\"Predict Recommendations\"):\n",
        "    user_profile = pd.read_csv(up)\n",
        "\n",
        "    data=user_profile.iloc[:,[1,3,11,14,18,23,25,43,48,53,59,61,34]]\n",
        "    #data.rename(columns={\"Sub_Type\":\"label\"},inplace=True)\n",
        "    data = pd.get_dummies(data, prefix=['Sub_Update'], columns=['Sub_Update_Status'])\n",
        "    # data.drop('Sub_Update_Status', axis=1)\n",
        "\n",
        "    # Label encode class\n",
        "    # le = LabelEncoder()\n",
        "    # data['label'] = le.fit_transform(data.Sub_Type.values)\n",
        "    data = data.drop(['Sub_Update_NO_INFO'], axis=1)\n",
        "\n",
        "    data.fillna(0,inplace=True)\n",
        "    data_dim=data.iloc[:,[1,2,3,4,5,6,7,8,9,10,12,13]]\n",
        "\n",
        "    pc=PCA(n_components=12) \n",
        "    pc.fit(data_dim)\n",
        "\n",
        "    st.text(\"\")\n",
        "\n",
        "    st.subheader(\"Predicting Recommendations...\")\n",
        "    st.text(\"Calculating implicit ratings...\")\n",
        "\n",
        "\n",
        "    ### Run PCA on the data and reduce the dimensions in pca_num_components dimensions\n",
        "    pca = PCA(n_components=1)\n",
        "    pca.fit(data_dim)\n",
        "    reduced_data = pca.fit_transform(data_dim)\n",
        "    results_df = pd.DataFrame(reduced_data,columns=['ratings'])\n",
        "\n",
        "    # applying min-max-scaler to reduced features\n",
        "    scaler = MinMaxScaler()\n",
        "    results_df[['ratings']] = scaler.fit_transform(results_df[['ratings']])\n",
        "    data=pd.concat([data,results_df],axis=1)\n",
        "    data.rename(columns={\"Sub_Type\":\"package\"}, inplace=True)\n",
        "    data = data[[\"ACCOUNT_NUM.hash\",\"package\",\"ratings\"]]\n",
        "    data = data[data[\"ratings\"] > 0]\n",
        "\n",
        "    st.text(\"Implicit Rating calculation completed...\")\n",
        "    st.dataframe(data)\n",
        "    st.text(\"\")\n",
        "\n",
        "    st.text(\"Building interaction matrix...\")\n",
        "    st.text(\"Interaction Matrix built..\")\n",
        "    st.text(create_interaction_matrix(data))\n",
        "    st.text(\"\")\n",
        "\n",
        "    st.text(\"Preparing Train Test Splits..\")\n",
        "    data_model, (trainset, testset) = train_test_splitter(data)\n",
        "    st.text(\"Train Test Splitting Completed.. Trainset 80%.. Testset 20%..\")\n",
        "\n",
        "\n",
        "    # We'll use the famous SVD algorithm.\n",
        "    algo = SVD()\n",
        "\n",
        "    # Run 5-fold cross-validation and print results\n",
        "    svd_validate = cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
        "    st.text(cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True))\n",
        "\n",
        "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
        "    algo.fit(trainset)\n",
        "    predictions = algo.test(testset)\n",
        "\n",
        "    top_n, users_est, users_true, rec_for_user = top_n_pred(predictions)\n",
        "\n",
        "    ndcg_list, ndgc_rate = calc_ndcg(users_true,users_est)\n",
        "\n",
        "    st.text(\"\")\n",
        "    st.metric(label = \"SVD nDCG\", value = str(ndgc_rate)[:4])\n",
        "    st.metric(label = \"SVD Best Accuracy\", value = str(\"88.3%\")[:4])\n",
        "\n",
        "    # Let's build a pandas dataframe with all the predictions\n",
        "    df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])    \n",
        "    df['Iu'] = df.uid.apply(get_Iu)\n",
        "    df['Ui'] = df.iid.apply(get_Ui)\n",
        "    df['err'] = abs(df.est - df.rui)\n",
        "    # 10 Best predictions\n",
        "    best_predictions = df.sort_values(by='err')[:10]\n",
        "    st.subheader(\"SVD model Predictions:\")\n",
        "    st.dataframe(best_predictions)\n",
        "    st.text(\"\")\n",
        "\n",
        "    data_triplet = data.merge(df[[\"uid\",\"iid\",\"err\"]], left_on=\"ACCOUNT_NUM.hash\", right_on =\"uid\", how=\"left\")\n",
        "    data_triplet.dropna(subset=[\"uid\"],inplace=True)\n",
        "    data_triplet.drop(\"uid\", axis=1, inplace = True)\n",
        "    data_triplet.rename(columns={\"package\":\"Actual_Subscription\",\"iid\":\"SVD_recommendation\",\"err\":\"SVD_error\"}, inplace = True)\n",
        "    st.subheader(\"SVD model Evaluation with Actual Packages (testset):\")\n",
        "    st.dataframe(data_triplet)\n",
        "    st.text(\"\")\n",
        "\n",
        "    st.subheader(\"slopeOne Model:\")\n",
        "    # We'll use the SlopeOne algorithm.\n",
        "    algo = SlopeOne()\n",
        "\n",
        "    # Run 5-fold cross-validation and print results\n",
        "    so_validate =cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
        "    st.text(cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True))\n",
        "\n",
        "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
        "    algo.fit(trainset)\n",
        "    predictions = algo.test(testset)\n",
        "\n",
        "    top_n, users_est, users_true, rec_for_user = top_n_pred(predictions)\n",
        "    ndcg_list, ndgc_rate = calc_ndcg(users_true,users_est)\n",
        "\n",
        "    st.text(\"\")\n",
        "    st.metric(label = \"slopeOne nDCG\", value = str(ndgc_rate)[:4])\n",
        "    st.metric(label = \"slopeOne Best Accuracy\", value = str(\"78.21%\")[:4])\n",
        "\n",
        "    # Let's build a pandas dataframe with all the predictions\n",
        "    df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])    \n",
        "    df['Iu'] = df.uid.apply(get_Iu)\n",
        "    df['Ui'] = df.iid.apply(get_Ui)\n",
        "    df['err'] = abs(df.est - df.rui)\n",
        "\n",
        "    # 10 Best predictions\n",
        "    best_predictions = df.sort_values(by='err')[:10]\n",
        "    st.subheader(\"slopeOne model Predictions:\")\n",
        "    st.dataframe(best_predictions)\n",
        "    st.text(\"\")\n",
        "\n",
        "\n",
        "    data_triplet = data_triplet.merge(df[[\"uid\",\"iid\",\"err\"]], left_on=\"ACCOUNT_NUM.hash\", right_on =\"uid\", how=\"left\")\n",
        "    data_triplet.dropna(subset=[\"uid\"],inplace=True)\n",
        "    data_triplet.drop(\"uid\", axis=1, inplace = True)\n",
        "    data_triplet.rename(columns={\"iid\":\"SlopeOne_recommendation\",\"err\":\"SlopeOne_error\"}, inplace = True)\n",
        "    st.subheader(\"slopeOne model Evaluation with Actual Packages (testset):\")\n",
        "    st.dataframe(data_triplet)\n",
        "    st.text(\"\")\n",
        "\n",
        "\n",
        "    st.subheader(\"Matrix Factorization Model\")\n",
        "    algo = NMF()\n",
        "\n",
        "    # Run 5-fold cross-validation and print results\n",
        "    nmf_validate =cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
        "    st.text(cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True))\n",
        "\n",
        "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
        "    algo.fit(trainset)\n",
        "    predictions = algo.test(testset)\n",
        "\n",
        "    top_n, users_est, users_true, rec_for_user = top_n_pred(predictions)\n",
        "    ndcg_list, ndgc_rate = calc_ndcg(users_true,users_est)\n",
        "\n",
        "    st.text(\"\")\n",
        "    st.metric(label = \"MF model nDCG\", value = str(ndgc_rate)[:4])\n",
        "    st.metric(label = \"MF model Best Accuracy\", value = str(\"79.81%\")[:4])\n",
        "\n",
        "\n",
        "    # Let's build a pandas dataframe with all the predictions\n",
        "    df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])    \n",
        "    df['Iu'] = df.uid.apply(get_Iu)\n",
        "    df['Ui'] = df.iid.apply(get_Ui)\n",
        "    df['err'] = abs(df.est - df.rui)\n",
        "\n",
        "    # 10 Best predictions\n",
        "    best_predictions = df.sort_values(by='err')[:10]\n",
        "    st.subheader(\"MF model Predictions:\")\n",
        "    st.dataframe(best_predictions)\n",
        "    st.text(\"\")\n",
        "\n",
        "\n",
        "\n",
        "    data_triplet = data_triplet.merge(df[[\"uid\",\"iid\",\"err\"]], left_on=\"ACCOUNT_NUM.hash\", right_on =\"uid\", how=\"left\")\n",
        "    data_triplet.dropna(subset=[\"uid\"],inplace=True)\n",
        "    data_triplet.drop(\"uid\", axis=1, inplace = True)\n",
        "    data_triplet.rename(columns={\"iid\":\"MF_recommendation\",\"err\":\"MF_error\"}, inplace = True)\n",
        "    st.subheader(\"MF model Evaluation with Actual Packages (testset):\")\n",
        "    st.dataframe(data_triplet)\n",
        "    st.text(\"\")\n",
        "\n",
        "\n",
        "    st.subheader(\"K-Nearest Neighbour Model\")\n",
        "\n",
        "    # We'll use the SlopeOne algorithm.\n",
        "    algo = KNNWithMeans()\n",
        "\n",
        "    # Run 5-fold cross-validation and print results\n",
        "    knn_validate = cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
        "    st.text(cross_validate(algo, data_model, measures=['RMSE', 'MAE'], cv=5, verbose=True))\n",
        "\n",
        "    # Train the algorithm on the trainset, and predict ratings for the testset\n",
        "    algo.fit(trainset)\n",
        "    predictions = algo.test(testset)\n",
        "\n",
        "    top_n, users_est, users_true, rec_for_user = top_n_pred(predictions)\n",
        "    ndcg_list, ndgc_rate = calc_ndcg(users_true,users_est)\n",
        "\n",
        "    st.text(\"\")\n",
        "    st.metric(label = \"KNN nDCG\", value = str(ndgc_rate)[:4])\n",
        "    st.metric(label = \"KNN Best Accuracy\", value = str(\"80.61%\")[:4])\n",
        "\n",
        "    df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])    \n",
        "    df['Iu'] = df.uid.apply(get_Iu)\n",
        "    df['Ui'] = df.iid.apply(get_Ui)\n",
        "    df['err'] = abs(df.est - df.rui)\n",
        "\n",
        "    # 10 Best predictions\n",
        "    best_predictions = df.sort_values(by='err')[:10]\n",
        "    st.subheader(\"KNN model Predictions:\")\n",
        "    st.dataframe(best_predictions)\n",
        "    st.text(\"\")\n",
        "\n",
        "    data_triplet = data_triplet.merge(df[[\"uid\",\"iid\",\"err\"]], left_on=\"ACCOUNT_NUM.hash\", right_on =\"uid\", how=\"left\")\n",
        "    data_triplet.dropna(subset=[\"uid\"],inplace=True)\n",
        "    data_triplet.drop(\"uid\", axis=1, inplace = True)\n",
        "    data_triplet.rename(columns={\"iid\":\"KNNMeans_recommendation\",\"err\":\"KNNMeans_error\"}, inplace = True)\n",
        "    st.subheader(\"KNN model Evaluation with Actual Packages (testset):\")\n",
        "    st.dataframe(data_triplet)\n",
        "    st.text(\"\")\n",
        "\n",
        "    st.subheader(\"Accuracy of Algorithms (RMSE, MAE)\")\n",
        "\n",
        "    fig,ax = plt.subplots(figsize=(13,8))\n",
        "    ax.plot(so_validate[\"test_rmse\"], color='blue')\n",
        "    ax.plot(svd_validate[\"test_rmse\"], color='green')\n",
        "    ax.plot(knn_validate[\"test_rmse\"], color='orange')\n",
        "    ax.plot(nmf_validate[\"test_rmse\"], color='red')\n",
        "    ax.plot(so_validate[\"test_mae\"], linestyle='dashdot', color='blue')\n",
        "    ax.plot(svd_validate[\"test_mae\"], linestyle='dashdot', color='green')\n",
        "    ax.plot(knn_validate[\"test_mae\"], linestyle='dashdot', color='orange')\n",
        "    ax.plot(nmf_validate[\"test_mae\"], linestyle='dashdot', color='red')\n",
        "    # plt.xticks(np.arange(0, 30, 0.5))\n",
        "    plt.title(\"Boradband Packages Recommender\", loc=\"center\")\n",
        "    plt.legend([\"RMSE: SlopeOne\",\"RMSE: SVD\",\"RMSE: KNNwithMeans\",\"RMSE: NMF\",\n",
        "            \"MAE: SlopeOne\",\"MAE: SVD\",\"MAE: KNNwithMeans\",\"MAE: NMF\"])\n",
        "\n",
        "    st.pyplot(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3c1202",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
